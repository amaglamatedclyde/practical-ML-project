
---


---
<div style="text-align:center;">
##Classification of the Qualitative Evaluation of
##Weight Lifting Exercises
###*Clyde Tressler*
January 10, 2016
</div>
####Summary
We present an analysis of the classification of qualitative evaluations of weight-lifting exercises. 
A boosted trees algorithm was trained on the [Weight Lifting Exercises Dataset](http://groupware.les.inf.puc-rio.br/har). The model scored perfectly on a hold-out test set, with 0 misclassifications and a **95% confidence interval of (0.9981, 1)**. The model also scored **20 out of 20 on its first attempt** at the Course Project Prediction Quiz.[^mynote1] 

[^mynote1]: This is astounding to me, but you can try it yourself thanks to Reproducible Research.

####A Note on Terminology
In this report we use the terminology **validation set** and **test set** to refer to a *tuning* and *hold-out* set respectively. 
This is consistent with </br> 
**[The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)** (Hastie et al.), widely recognized as a seminal text in the field. From page 222:
<div style="text-align:center;">
</br>
*"The training set is used to fit the models; the validation
set is used to estimate prediction error for model selection; the test set is
used for assessment of the generalization error of the final chosen model."*
</div>

####Background
<p>The data were originally collected and analyzed in this paper:</p>

**Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.**

The authors describe the dataset as follows:

<div text-align="center">
<p>*"Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes."*</p>
</div>
Here we show a table of the occurrences of the five classes in the training set. Note that the 'mistake' classes have similar sample numbers and the 'correctly performed' class is greater than any other individual class. 

```{r, echo=FALSE, message=FALSE}
library(dplyr)
setwd("/home/clyde/practical-ML-project")
train.valid <- read.csv("pml-training.csv")
table(train.valid$classe)
```

####Exploring and Tidying the Data

Exploration of the data show many sparse columns for which 97.93% of the values are NAs. These variables will not be included in the analysis. For brevity, the R code for these cleaning steps has not been included here but can be found in this [github repository](https://github.com/amaglamatedclyde/practical-ML-project). 

The remaining variables are free of NA values and are considered to be candidates for inclusion in the analysis. Because we intend to use a boosted tree algorithm, collinearity of these variables is not investigated at this step.

```{r, echo=F, include=FALSE}
#here we check to see the frequency of occurences of NA in the columns as a percentage. 
f <- function(c){all(is.na(c))}
f2 <- function(c){any(is.na(c))}
apply(train.valid, 2, f)
any(apply(train.valid, 2, f2)) #any NAs? no!
any(apply(train.valid, 2, f)==TRUE)
#find Na %
f1 <- function(c){sum(is.na(c))/length(c)*100}
apply(train.valid, 2, f1)
apply(train.valid, 2, f1) >50
sum(apply(train.valid, 2, f1) >50)
#lots of columns with a lot of NAs all >97%
a <- apply(train.valid, 2, f1)
#lots of columns with a lot of NAs all >97%
sum(a>0&a<97)
many.na <- names(which(a>97))
train.valid <- select(train.valid, -one_of(many.na))
```

Next we observe that the column labeled 'X' is merely an index number and therefor not a valid predictor. Several variables are timestamps. Since we are interested in classifying individual observations, these timestamp columns can also be excluded. Also, we search the columns for near zero variance variables and threshold this at a conservative frequency ratio of 60 to eliminate the following predictors:[^mynote2]

[^mynote2]: "Near-zero variance means that the fraction of unique values over the sample size is low (say 10%)... and the ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say **around 20**). If both of these criteria are true and the model in question is susceptible to this type of predictor, it may be advantageous to remove the variable from the model." (Kuhn, M., & Johnson, K. (2013). **Applied Predictive Modeling**, New York, NY: Springer)


```{r, include=F}
sum(train.valid$new_window=='no')
sum(train.valid$new_window=='no')/nrow(train.valid)
train.valid <- select(train.valid, -new_window)
f3 <- function(c){levels(c)==2}
apply(train.valid, 2, f3)
#X is just a row number
train.valid <- select(train.valid, -X)
#we don't want time-dependent data
library(caret)
train.valid <- select(train.valid, -contains('timestamp'))
#leave out nzv vars
nzv <- nearZeroVar(train.valid[-88], saveMetrics= TRUE) #find nzv vars
nzv <- subset(nzv, freqRatio>60&nzv) #threshold nzv vars
train.valid <- select(train.valid, -one_of(rownames(nzv)))
```

```{r, echo=F}
nzv
```

####Training the Classifier
Because of the large number of observations, we choose a **training, validation, test set** approach over cross-validation.

An examination of the dataset shows that is has been sorted according to the class outcome. We therefor use the following methodology to randomly sample the rows, creating a training, validation and test set to build and evaluate our model. Here, as mentioned previously, we use the validation set to tune the model and the testing set as our hold-out set. We choose a validation set that is nearly twice the size of the testing set so we can evaluate the bias of the model using the larger number of samples in the validation set and the variance of the model using the smaller number of observations in the testing set.

```{r, output="hide"}
set.seed(314159)
unloadNamespace('dplyr')
include <- sample(c(FALSE, TRUE), size=nrow(train.valid), replace=T, prob=c(0.1, 0.9))
training <- train.valid[include,]
testing <- train.valid[!include,]
include <- sample(c(FALSE, TRUE), size=nrow(training), replace=T, prob=c(0.2, 0.8))
train <- training[include,]
validation <- training[!include,]
```

```{r, include=F, cache=T, message=F}
library(caret)
training <- train
# which(names(training2)=="num_window")
# training <- training[-2]
# fitControl <- trainControl(## 10-fold CV
#                            method = "cv",
#                            number = 10)fit1 <- system.time(train(classe~., data=training, method="xgbTree"))
#                            ## repeated ten times
#                            # repeats = 10)
# fit1 <- system.time(train(classe~., data=training, method="xgbTree"))
#     user   system  elapsed 
# 8971.363    5.527  796.503
fit1 <- train(classe~., data=training, method="xgbTree")
preds <- predict(fit1, validation)
```
####Choosing an Algorithm
Our initial choice of an algorithm for our classification model is [Extreme Gradient Boosting](http://startup.ml/blog/xgboost), which has garnered a reputation for extraordinary out-of-the-box **(OOB)** performance on [kaggle.com](http://kaggle.com) competitions. 

We train on a set of **14220** observations in the **training set** and evaluate predictions on the **3473** observations in the validation set. The confusion matrix for the validation set is shown below. Note that there are only 2 misclasssifications.[^mynote3]

[^mynote3]: This is astounding to me, but you can try it yourself thanks to Reproducible Research.


```{r, echo=F}
confusionMatrix(preds, validation$classe)
```
####Performance
This investigator was fully prepared to cross-validate model tuning parameters and stack multiple models to improve prediction[^mynote4]. However, the **99.94% accuracy on the validation set** achieved using the xgboost algorithm with *default* settings led directly to an assessment on the test set. 

The test set performed even better, with 0 misclassifications. The **95% confidence interval** was shown to be **(0.9981, 1) on the hold-out test set of 1929 observations.** This gives a corresponding **out-of-sample error rate interval of (0,0.0019).**

At this point it seemed foolish not to attempt the prediction portion of the course assessment. This resulted in 20 correct predictions out of 20 trials.

[^mynote4]: Seriously- you can't get beter than accuracy of 1 (see test set accuracy). Aside from the pleasure of an intellectual pursuit, why go further? 

The confusion matrix for the test set is shown here.

```{r, echo=F, message=F}
preds <- predict(fit1, testing)
confusionMatrix(preds, testing$classe)
```

The xgbTree algorithm is well-optimized for speed and fully utilized all 6 hyper-threaded cores (12 virtual CPUs) during the model calculation. Memory use was modest, around 5GB. The elapsed time of the computation was approximately 13 minutes.

Admittedly, the success of this effort is mostly attributable to the power of the XGBoost algorithm, elements of which are something of a black box to this investigator[^mynote5]. It can be assumed, however, that the predictor selection process (my part) was also crucial to the success of the algorithm.

[^mynote5]:Here is another interesting article on gradient boosting: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/

####A Final Note on the Prediction Assignment
The 20 observations in the set of prediction observations were found to have NA values for some factor variables. These turned out to be sparse columns that were not eliminated in our exploratory phase. A choice presented itself: relevel the test observations or eliminate the factor variables in question from the model. A new model without the factor variables was evaluated against the validation set and was shown to have the same accuracy, so this was the final model used for the prediction portion of this assignment. This part of the assignment has not been included here.

```{r test observations, include=FALSE}
######test data
######here we remove the factor columns and fit a new model. accuracy stays the same
# names(testing)
# library(dplyr)
# sapply(training[which(factors==T)], levels)
# factors <- which(factors==T)
# factors <- factors[-64]
# factors <- factors[-1]
# training2 <- training[-factors]
# fit2 <- train(classe~., data=training2, method="xgbTree")
# n <- names(training2)
# n <- n[-55]
# test <- select(test, one_of(n))
```
